# Implementaci√≥n de Chat Multi-Proveedor

## Resumen de Cambios

Se ha implementado un sistema completo de chat que soporta m√∫ltiples proveedores de IA (OpenAI, Anthropic, Google AI, OpenRouter) con streaming en tiempo real y caracter√≠sticas espec√≠ficas para cada proveedor.

## Problema Solucionado

### üö´ **Antes:**
```
API integration for OpenAI is not yet implemented. 
Please select an Ollama model to continue.
```

### ‚úÖ **Ahora:**
- **Chat funcional** con OpenAI, Anthropic, Google AI y OpenRouter
- **Streaming en tiempo real** para todos los proveedores
- **Configuraci√≥n persistente** de API keys y modelos
- **Manejo espec√≠fico** de cada proveedor seg√∫n sus caracter√≠sticas

## Arquitectura Implementada

### 1. **Servicios de Chat por Proveedor**
Cada proveedor tiene su implementaci√≥n espec√≠fica optimizada para sus caracter√≠sticas:

```
src/services/chat/
‚îú‚îÄ‚îÄ types.ts         # Interfaces comunes
‚îú‚îÄ‚îÄ openai.ts        # Implementaci√≥n OpenAI
‚îú‚îÄ‚îÄ anthropic.ts     # Implementaci√≥n Anthropic  
‚îú‚îÄ‚îÄ google.ts        # Implementaci√≥n Google AI
‚îú‚îÄ‚îÄ openrouter.ts    # Implementaci√≥n OpenRouter
‚îî‚îÄ‚îÄ index.ts         # Gesti√≥n centralizada
```

### 2. **Caracter√≠sticas por Proveedor**

#### **OpenAI (openai.ts)**
- **API:** `https://api.openai.com/v1/chat/completions`
- **Streaming:** Server-Sent Events con `data: [DONE]`
- **Modelos:** GPT-4, GPT-3.5-turbo, etc.
- **Headers:** `Authorization: Bearer ${apiKey}`

#### **Anthropic (anthropic.ts)**
- **API:** `https://api.anthropic.com/v1/messages`
- **Streaming:** Server-Sent Events con tipos de mensaje
- **Formato especial:** Conversi√≥n de mensajes sistema/usuario/asistente
- **Headers:** `x-api-key`, `anthropic-version: 2023-06-01`

#### **Google AI (google.ts)**
- **API:** `https://generativelanguage.googleapis.com/v1beta/models/{model}`
- **Streaming:** SSE con `?alt=sse` para streaming
- **Formato √∫nico:** `contents` y `systemInstruction` separados
- **Headers:** `x-goog-api-key`

#### **OpenRouter (openrouter.ts)**
- **API:** `https://openrouter.ai/api/v1/chat/completions`
- **Streaming:** Compatible con OpenAI
- **Headers especiales:** `HTTP-Referer`, `X-Title` (requeridos)
- **Modelo din√°mico:** Puede cambiar el modelo en respuesta

## Implementaci√≥n T√©cnica

### 1. **Tipos Base (types.ts)**
```typescript
interface ChatMessage {
  role: 'user' | 'assistant' | 'system'
  content: string
}

interface ChatResponse {
  response: string
  metadata: {
    model: string
    provider: string
    tokens?: number
    usage?: TokenUsage
    finish_reason?: string
  }
}

interface ChatProvider {
  name: string
  sendMessage: (model, messages, apiKey, onStream?) => Promise<ChatResponse>
  supportsStreaming: boolean
}
```

### 2. **Gesti√≥n Centralizada (index.ts)**
```typescript
export const chatProviders: Record<string, ChatProvider> = {
  'openai': openaiProvider,
  'anthropic': anthropicProvider,
  'google': googleProvider,
  'openrouter': openrouterProvider,
}

export async function sendChatMessage(
  providerName: string,
  model: string,
  messages: ChatMessage[],
  apiKey: string,
  onStream?: (chunk: string) => void
): Promise<ChatResponse>
```

### 3. **Integraci√≥n con ChatStore**
```typescript
// Nuevo m√©todo en chatStore
callAPIProvider: async (provider, model, prompt, context) => {
  // 1. Obtener configuraci√≥n del proveedor desde aiConfigStore
  // 2. Validar API key y estado de conexi√≥n
  // 3. Convertir mensajes al formato ChatMessage
  // 4. Llamar sendChatMessage con streaming
  // 5. Retornar respuesta formateada
}
```

## Flujo de Conversaci√≥n

### 1. **Usuario Env√≠a Mensaje**
```
User Input ‚Üí ChatStore.sendMessage()
```

### 2. **Selecci√≥n de Proveedor**
```typescript
if (conversation.provider === 'Ollama') {
  // Usar implementaci√≥n local existente
  callOllamaAPI()
} else {
  // Usar nuevo sistema multi-proveedor
  callAPIProvider(provider, model, prompt, context)
}
```

### 3. **Procesamiento por Proveedor**
```
callAPIProvider() ‚Üí 
  ‚Üì Buscar configuraci√≥n en aiConfigStore
  ‚Üì Validar API key y estado
  ‚Üì Convertir mensajes
  ‚Üì sendChatMessage(provider, ...)
  ‚Üì Proveedor espec√≠fico (openai.ts, anthropic.ts, etc.)
  ‚Üì Streaming chunks ‚Üí setStreamingMessage()
  ‚Üì Respuesta final ‚Üí addMessageToConversation()
```

## Caracter√≠sticas Implementadas

### ‚úÖ **Streaming Universal**
- **Todos los proveedores** soportan streaming
- **Throttling inteligente** (30-80ms delay) para UX suave
- **Manejo de errores** espec√≠fico por proveedor

### ‚úÖ **Configuraci√≥n Persistente**
- **API keys** se obtienen de `aiConfigStore`
- **Validaci√≥n autom√°tica** de estado de conexi√≥n
- **Error messaging** claro si falta configuraci√≥n

### ‚úÖ **Formato de Mensajes Espec√≠fico**

#### **OpenAI/OpenRouter:** Directo
```typescript
messages: [
  { role: 'user', content: 'Hello' },
  { role: 'assistant', content: 'Hi!' }
]
```

#### **Anthropic:** Conversi√≥n especial
```typescript
// Sistema ‚Üí Prepend a primer mensaje usuario
// Alternancia user/assistant estricta
```

#### **Google AI:** Formato √∫nico
```typescript
{
  contents: [{ role: 'user', parts: [{ text: 'Hello' }] }],
  systemInstruction: { parts: [{ text: 'System prompt' }] }
}
```

### ‚úÖ **Metadata Rica**
- **Tokens usados** (prompt + completion + total)
- **Raz√≥n de finalizaci√≥n** (stop, length, etc.)
- **Modelo real usado** (importante para OpenRouter)
- **Timestamps** precisos

## Casos de Uso Solucionados

### ‚úÖ **Chat con OpenAI**
1. Usuario configura API key de OpenAI
2. Selecciona modelo GPT-4
3. **Ahora puede chatear** con streaming funcional
4. Metadata muestra tokens usados

### ‚úÖ **Chat con Anthropic Claude**
1. Usuario configura API key de Anthropic
2. Selecciona Claude 3.5 Sonnet
3. **Mensajes se convierten** al formato Anthropic
4. **Streaming funciona** con tipos de evento espec√≠ficos

### ‚úÖ **Chat con Google Gemini**
1. Usuario configura API key de Google AI
2. Selecciona Gemini Pro
3. **Formato se adapta** a la API de Google
4. **Instrucciones sistema** se manejan separadamente

### ‚úÖ **Chat con OpenRouter**
1. Usuario configura API key de OpenRouter
2. Agrega modelos personalizados (ej: `anthropic/claude-3-haiku`)
3. **Headers espec√≠ficos** incluidos autom√°ticamente
4. **Modelo puede cambiar** en respuesta (tracked en metadata)

## Archivos Creados/Modificados

### **Nuevos:**
- `src/services/chat/types.ts` - Interfaces comunes
- `src/services/chat/openai.ts` - Proveedor OpenAI
- `src/services/chat/anthropic.ts` - Proveedor Anthropic
- `src/services/chat/google.ts` - Proveedor Google AI
- `src/services/chat/openrouter.ts` - Proveedor OpenRouter
- `src/services/chat/index.ts` - Gesti√≥n centralizada

### **Modificados:**
- `src/stores/chatStore.ts` - Integraci√≥n multi-proveedor

## Beneficios

- ‚úÖ **Chat funcional** con todos los proveedores principales
- ‚úÖ **Streaming consistente** en toda la aplicaci√≥n
- ‚úÖ **Manejo espec√≠fico** de caracter√≠sticas √∫nicas por proveedor
- ‚úÖ **Escalabilidad** f√°cil para agregar nuevos proveedores
- ‚úÖ **Error handling** robusto y espec√≠fico
- ‚úÖ **Configuraci√≥n integrada** con el sistema existente

## Consideraciones de Seguridad

### **API Keys**
- Almacenadas en `aiConfigStore` con persistencia
- Validadas antes de cada llamada
- No se exponen en logs (solo se logean longitudes)

### **Rate Limiting**
- Cada proveedor maneja sus propios l√≠mites
- Errores HTTP espec√≠ficos se propagan correctamente
- Timeouts configurables por proveedor

## Testing Recomendado

### **Para cada proveedor:**
1. **Configurar API key** v√°lida
2. **Activar proveedor** y testear conexi√≥n
3. **Seleccionar modelo** y enviarlo al chat
4. **Verificar streaming** funciona correctamente
5. **Probar manejo de errores** (API key inv√°lida, modelo inexistente)

### **Casos edge:**
- API key vac√≠a o inv√°lida
- Proveedor desactivado
- Modelo no soportado
- Network errors
- Rate limiting

## Pr√≥ximos Pasos Sugeridos

1. **Configuraci√≥n avanzada:** Temperature, max_tokens por proveedor
2. **Cache de respuestas:** Para mejorar performance
3. **M√©tricas de uso:** Tracking de tokens y costos
4. **Fallback providers:** Sistema de respaldo autom√°tico
5. **Custom prompts:** Sistema prompts por proveedor